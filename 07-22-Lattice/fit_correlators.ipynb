{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------------------------------------------------------------------------\n",
    "# basic setup of the notebook\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "from scipy import *\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# everything in iminuit is done through the Minuit object, so we import it\n",
    "from iminuit import Minuit\n",
    "from iminuit.util import describe\n",
    "from typing import Annotated\n",
    "\n",
    "# display iminuit version\n",
    "import iminuit\n",
    "print(\"iminuit version:\", iminuit.__version__)\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------------------------------------------------------------------------\n",
    "# Importing fixed params for analysis\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------  \n",
    "total_size_lat = 32\n",
    "measured_T     = 16\n",
    "xi             = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------------------------------------------------------------------------\n",
    "# Read raw data and prepare accordingly\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------\n",
    "data=[]\n",
    "data.append(np.genfromtxt('data/L{}_xi{}.dat'.format(total_size_lat,xi))[:,1])\n",
    "data=np.array(data[0])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, store the data in a convenient format for the fits\n",
    "\n",
    "totaltraj=int(len(data)/measured_T)            # Number of montecarlo samples\n",
    "\n",
    "Gc=np.zeros((int(totaltraj),measured_T))       # Storing the data\n",
    "for i in range(int(totaltraj)):\n",
    "    Gc[[i]]=data[range(i*measured_T,(i+1)*measured_T)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We read the data, lets create a class for Jackknife sampling\n",
    "\n",
    "def mylen(x):\n",
    "    return len(x) if isinstance(x, np.ndarray) or isinstance(x, list) else 1\n",
    "\n",
    "class jackknife:\n",
    "\n",
    "    def __init__(self, list):\n",
    "        self.list=list\n",
    "        self.Lj=len(list)\n",
    "        self.Nj=self.Lj\n",
    "        Nt=mylen(self.list[0])\n",
    "        jackk=np.zeros((self.Nj-1,Nt))\n",
    "        self.jackkf=np.zeros((self.Nj,Nt))\n",
    "        for i in range(self.Nj):\n",
    "            jackk            = np.delete(self.list,i,axis=0)\n",
    "            self.jackkf[i]   = np.sum(jackk,axis=0)/(self.Nj-1)\n",
    "\n",
    "    #sample values: jackknife(original_ensemble).sample()=jackknife_ensemble\n",
    "    def sample(self):\n",
    "        return self.jackkf    \n",
    "\n",
    "    def fcov(self):\n",
    "        return np.cov(self.jackkf,rowvar=False,bias=True) * (self.Nj-1)     \n",
    "    \n",
    "    #augmented sample: jackknife(jackknife_ensemble).up()=original_ensemble\n",
    "    def up(self):\n",
    "        Lj=len(self.list)\n",
    "        mean=np.mean(self.list,axis=0)\n",
    "        ensem=self.list+(Lj)*(mean-self.list)\n",
    "        return ensem    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now lets code another class to compute means and covariances\n",
    "class ensemble_stat:\n",
    "\n",
    "    def __init__(self, list):\n",
    "        self.list=list\n",
    "\n",
    "    def mean(self):\n",
    "        mean=np.mean(self.list,axis=0)\n",
    "        return mean  \n",
    "\n",
    "    #reduced variance\n",
    "    def rcov(self):\n",
    "        Nj=len(self.list)\n",
    "        cov=np.cov(self.list, rowvar=False,bias=True) / (Nj-1)\n",
    "        return cov    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the basic IMinuit penalty function class, lets modify it to include correlations\n",
    "\n",
    "class LeastSquares:\n",
    "    \"\"\"\n",
    "    Generic least-squares cost function with cov.\n",
    "    \"\"\"\n",
    "\n",
    "    errordef = Minuit.LEAST_SQUARES  # for Minuit to compute errors correctly\n",
    "\n",
    "    def __init__(self, model, x, y, incov): # IMPORTANT: We are reading the inverse of a covariance matrix\n",
    "        self.model = model  # model predicts y for given x\n",
    "        self.x = np.asarray(x)\n",
    "        self.y = np.asarray(y)\n",
    "        self.invcov = np.asarray(incov)\n",
    "\n",
    "    def __call__(self, *par):  # we must accept a variable number of model parameters\n",
    "        ym  = self.model(self.x, *par)\n",
    "        fun = np.dot(np.dot((self.y - ym), self.invcov),(self.y - ym))\n",
    "\n",
    "        return fun\n",
    "        \n",
    "class BetterLeastSquares(LeastSquares):\n",
    "\n",
    "    def __init__(self, model, x, y, incov):\n",
    "        super().__init__(model, x, y, incov)\n",
    "        pars = describe(model, annotations=True)\n",
    "        model_args = iter(pars)\n",
    "        next(model_args)\n",
    "        _parameters = {k: pars[k] for k in model_args}\n",
    "\n",
    "\n",
    "class EvenBetterLeastSquares(BetterLeastSquares):\n",
    "    @property\n",
    "    def ndata(self):\n",
    "        return len(self.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's finish by defining a function as a sum of exponentials\n",
    "\n",
    "def exp_np(t,*pars):\n",
    "    total=0\n",
    "    mass=np.abs(pars[1])\n",
    "    total+=np.abs(pars[0])*np.exp(-mass*t)\n",
    "    for i in range(1,np.int_((len(pars)+1)/2)):\n",
    "        mass+=np.abs(pars[2*i+1])\n",
    "        total+=np.abs(pars[2*i])*np.exp(-mass*t)\n",
    "    return total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, lets start with a simple plot and fit test to some specific data\n",
    "\n",
    "mcalls=5000\n",
    "mtol=0.0001\n",
    "\n",
    "\n",
    "jackk_C_r_t=jackknife(Gc).sample()                             # This is our main data sample for a fixed R     \n",
    "lt=len(Gc[0])\n",
    "dfin=min(lt,100)\n",
    "\n",
    "tfin=3\n",
    "tffin=16\n",
    "\n",
    "data_t   = np.linspace(tfin, tffin, tffin-tfin+1)                                     # Creating x axis values\n",
    "data_y   = ensemble_stat(jackk_C_r_t).mean()[tfin-1:tffin]                        # Obtaining the data averages over the MonteCarlo samples\n",
    "data_cov = jackknife(Gc).fcov() #ensemble_stat(jackknife(jackk_C_r_t).up()).rcov()          # Obtaining the covariance matrix\n",
    "data_err = np.sqrt(np.diagonal(data_cov))[tfin-1:tffin]                              # This is the error from the covariance matrix\n",
    "data_incov=np.linalg.inv(data_cov)[tfin-1:tffin,tfin-1:tffin]\n",
    "\n",
    "\n",
    "funfit=exp_np                  # Model chosen to fit the C(R,t) for varying time t, fixed R\n",
    "inipars=np.array([1.,1.])           # Initial parameters for the model to fit\n",
    "\n",
    "least_squares_np = EvenBetterLeastSquares(funfit, data_t, data_y, data_incov)\n",
    "m=Minuit(least_squares_np,*inipars)\n",
    "\n",
    "m.migrad(mcalls).migrad(mcalls).hesse(mcalls)\n",
    "\n",
    "chi2_total=m.fval\n",
    "\n",
    "data_t_plot=np.linspace(data_t[0],data_t[-1],1000)\n",
    "plt.errorbar(data_t, data_y, data_err, fmt=\"ok\", label=\"data\")\n",
    "plt.plot(data_t_plot, funfit(data_t_plot, *m.values), label=\"fit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can you repeat the exercise, but without correlations?\n",
    "\n",
    "mcalls=5000\n",
    "mtol=0.0001\n",
    "\n",
    "totaltraj=int(len(data)/measured_T)\n",
    "\n",
    "jackk_C_r_t=jackknife(Gc).sample()                             # This is our main data sample for a fixed R     \n",
    "lt=len(Gc[0])\n",
    "dfin=min(lt,100)\n",
    "\n",
    "tfin=3\n",
    "tffin=16\n",
    "\n",
    "data_t   = np.linspace(tfin, tffin, tffin-tfin+1)                                     # Creating x axis values\n",
    "data_y   = ensemble_stat(jackk_C_r_t).mean()[tfin-1:tffin]                        # Obtaining the data averages over the MonteCarlo samples\n",
    "data_cov = jackknife(Gc).fcov() #ensemble_stat(jackknife(jackk_C_r_t).up()).rcov()          # Obtaining the covariance matrix\n",
    "data_cov = np.diag(np.diag(data_cov))\n",
    "data_err = np.sqrt(np.diagonal(data_cov))[tfin-1:tffin]                              # This is the error from the covariance matrix\n",
    "data_incov=np.linalg.inv(data_cov)[tfin-1:tffin,tfin-1:tffin]\n",
    "\n",
    "\n",
    "funfit=exp_np                  # Model chosen to fit the C(R,t) for varying time t, fixed R\n",
    "inipars=np.array([1.,1.])           # Initial parameters for the model to fit\n",
    "\n",
    "least_squares_np = EvenBetterLeastSquares(funfit, data_t, data_y, data_incov)\n",
    "m2=Minuit(least_squares_np,*inipars)\n",
    "\n",
    "m2.migrad(mcalls).migrad(mcalls).hesse(mcalls)\n",
    "\n",
    "\n",
    "data_t_plot=np.linspace(data_t[0],data_t[-1],1000)\n",
    "plt.errorbar(data_t, data_y, data_err, fmt=\"ok\", label=\"data\")\n",
    "plt.plot(data_t_plot, funfit(data_t_plot, *m.values), label=\"fit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare now both central values and errors for the ground state energy, are they compatible?\n",
    "\n",
    "print(np.abs(np.array(m.values))[1],np.abs(np.array(m.errors))[1])\n",
    "print(np.abs(np.array(m2.values))[1],np.abs(np.array(m2.errors))[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's now produce a Jackknife set of fits to data\n",
    "\n",
    "mcalls=5000\n",
    "mtol=0.0001\n",
    "\n",
    "jackk_C_r_t=jackknife(Gc).sample()                             # This is our main data sample for a fixed R     \n",
    "lt=len(Gc[0])\n",
    "dfin=min(lt,100)\n",
    "\n",
    "tfin=3\n",
    "tffin=16\n",
    "\n",
    "data_t   = np.linspace(tfin, tffin, tffin-tfin+1)                                     # Creating x axis values\n",
    "data_cov = jackknife(Gc).fcov() #ensemble_stat(jackknife(jackk_C_r_t).up()).rcov()          # Obtaining the covariance matrix\n",
    "data_err = np.sqrt(np.diagonal(data_cov))[tfin-1:tffin]                              # This is the error from the covariance matrix\n",
    "data_incov=np.linalg.inv(data_cov)[tfin-1:tffin,tfin-1:tffin]\n",
    "\n",
    "\n",
    "funfit=exp_np                  # Model chosen to fit the C(R,t) for varying time t, fixed R\n",
    "inipars=np.array([1.,1.])           # Initial parameters for the model to fit\n",
    "\n",
    "\n",
    "E_0=np.zeros(totaltraj)\n",
    "chi2_jackk=np.zeros(totaltraj)\n",
    "for jackk in range(totaltraj):\n",
    "    data_y   = (jackk_C_r_t[jackk])[tfin-1:tffin]                        # Obtaining the data averages over the Montecarlo samples\n",
    "\n",
    "    least_squares_np = EvenBetterLeastSquares(funfit, data_t, data_y, data_incov)\n",
    "\n",
    "    jackk_fit=Minuit(least_squares_np,*inipars).migrad(mcalls).migrad(mcalls).hesse(mcalls)\n",
    "\n",
    "    E_0[jackk]        = np.array(jackk_fit.values)[1]\n",
    "    chi2_jackk[jackk] = jackk_fit.fval "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, let's compare this value with the simple fit to data\n",
    "\n",
    "print(ensemble_stat(E_0).mean(),np.sqrt(jackknife(jackknife(E_0).up()).fcov()))\n",
    "\n",
    "print(np.abs(np.array(m.values))[1],np.abs(np.array(m.errors))[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXTRA: Use the formula introduced in the lectures for the relation between the total chi2 and the Jackknife or Raw sample chi2's\n",
    "\n",
    "chi2_total\n",
    "\n",
    "ensemble_stat(chi2_jackk).mean()\n",
    "\n",
    "ensemble_stat(chi2_jackk).mean()-(tffin-tfin+1)/(totaltraj-1)\n",
    "\n",
    "ensemble_stat(chi2_jackk).mean()-(tffin-tfin+1-len(inipars))/(totaltraj-1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
